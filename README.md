<div align="center">

<img alt="simplemem_logo" src="https://github.com/user-attachments/assets/6ea54ad1-e007-442c-99d7-1174b10d1fec" width="450">

<div align="center">

## Efficient Lifelong Memory for LLM Agents

<small>Store, compress, and retrieve long-term memories with semantic lossless compression. Works across Claude, Cursor, LM Studio, and more.</small>

</div>

<p><b>Works with any AI platform that supports MCP or Python integration</b></p>

<table>
<tr>

<td align="center" width="100">
  <a href="https://www.anthropic.com/claude">
    <img src="https://cdn.simpleicons.org/claude/D97757" width="48" height="48" alt="Claude Desktop" />
  </a><br/>
  <sub>
    <a href="https://www.anthropic.com/claude"><b>Claude Desktop</b></a>
  </sub>
</td>

<td align="center" width="100">
  <a href="https://cursor.com">
    <picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://cdn.simpleicons.org/cursor/FFFFFF">
      <img src="https://cdn.simpleicons.org/cursor/000000" width="48" height="48" alt="Cursor" />
    </picture>
  </a><br/>
  <sub>
    <a href="https://cursor.com"><b>Cursor</b></a>
  </sub>
</td>

<td align="center" width="100">
  <a href="https://lmstudio.ai">
    <img src="https://github.com/lmstudio-ai.png?size=200" width="48" height="48" alt="LM Studio" />
  </a><br/>
  <sub>
    <a href="https://lmstudio.ai"><b>LM Studio</b></a>
  </sub>
</td>

<td align="center" width="100">
  <a href="https://cherry-ai.com">
    <img src="https://github.com/CherryHQ.png?size=200" width="48" height="48" alt="Cherry Studio" />
  </a><br/>
  <sub>
    <a href="https://cherry-ai.com"><b>Cherry Studio</b></a>
  </sub>
</td>

<td align="center" width="100">
  <a href="https://pypi.org/project/simplemem/">
    <img src="https://cdn.simpleicons.org/pypi/3775A9" width="48" height="48" alt="PyPI" />
  </a><br/>
  <sub>
    <a href="https://pypi.org/project/simplemem/"><b>PyPI Package</b></a>
  </sub>
</td>

<td align="center" width="100">
  <sub><b>+ Any MCP<br/>Client</b></sub>
</td>

</tr>
</table>

<div align="center">

<br/>

[ğŸ‡¨ğŸ‡³ ä¸­æ–‡](./docs/i18n/README.zh-CN.md) â€¢
[ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª](./docs/i18n/README.ja.md) â€¢
[ğŸ‡°ğŸ‡· í•œêµ­ì–´](./docs/i18n/README.ko.md) â€¢
[ğŸ‡ªğŸ‡¸ EspaÃ±ol](./docs/i18n/README.es.md) â€¢
[ğŸ‡«ğŸ‡· FranÃ§ais](./docs/i18n/README.fr.md) â€¢
[ğŸ‡©ğŸ‡ª Deutsch](./docs/i18n/README.de.md) â€¢
[ğŸ‡§ğŸ‡· PortuguÃªs](./docs/i18n/README.pt-br.md)<br/>
[ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹](./docs/i18n/README.ru.md) â€¢
[ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](./docs/i18n/README.ar.md) â€¢
[ğŸ‡®ğŸ‡¹ Italiano](./docs/i18n/README.it.md) â€¢
[ğŸ‡»ğŸ‡³ Tiáº¿ng Viá»‡t](./docs/i18n/README.vi.md) â€¢
[ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e](./docs/i18n/README.tr.md)

<br/>

[![Project Page](https://img.shields.io/badge/ğŸ¬_INTERACTIVE_DEMO-Visit_Our_Website-FF6B6B?style=for-the-badge&labelColor=FF6B6B&color=4ECDC4&logoColor=white)](https://aiming-lab.github.io/SimpleMem-Page)

<p align="center">
  <a href="https://arxiv.org/abs/2601.02553"><img src="https://img.shields.io/badge/arXiv-2601.02553-b31b1b?style=flat&labelColor=555" alt="arXiv"></a>
  <a href="https://github.com/aiming-lab/SimpleMem"><img src="https://img.shields.io/badge/github-SimpleMem-181717?style=flat&labelColor=555&logo=github&logoColor=white" alt="GitHub"></a>
  <a href="LICENSE"><img src="https://img.shields.io/github/license/aiming-lab/SimpleMem?style=flat&label=license&labelColor=555&color=2EA44F" alt="License"></a>
  <a href="https://github.com/aiming-lab/SimpleMem/pulls"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen?style=flat&labelColor=555" alt="PRs Welcome"></a>
  <br/>
  <a href="https://pypi.org/project/simplemem/"><img src="https://img.shields.io/pypi/v/simplemem?style=flat&label=pypi&labelColor=555&color=3775A9&logo=pypi&logoColor=white" alt="PyPI"></a>
  <a href="https://pypi.org/project/simplemem/"><img src="https://img.shields.io/pypi/pyversions/simplemem?style=flat&label=python&labelColor=555&color=3775A9&logo=python&logoColor=white" alt="Python"></a>
  <a href="https://mcp.simplemem.cloud"><img src="https://img.shields.io/badge/MCP-mcp.simplemem.cloud-14B8A6?style=flat&labelColor=555" alt="MCP Server"></a>
  <a href="https://github.com/aiming-lab/SimpleMem"><img src="https://img.shields.io/badge/Claude_Skills-supported-FFB000?style=flat&labelColor=555" alt="Claude Skills"></a>
  <br/>
  <a href="https://discord.gg/KA2zC32M"><img src="https://img.shields.io/badge/Discord-Join_Chat-5865F2?style=flat&labelColor=555&logo=discord&logoColor=white" alt="Discord"></a>
  <a href="fig/wechat_logo3.JPG"><img src="https://img.shields.io/badge/WeChat-Group-07C160?style=flat&labelColor=555&logo=wechat&logoColor=white" alt="WeChat"></a>
</p>

<br/>

[Overview](#-overview) â€¢ [Quick Start](#-quick-start) â€¢ [Cross-Session Memory](#-cross-session-memory) â€¢ [MCP Server](#-mcp-server) â€¢ [Evaluation](#-evaluation) â€¢ [Citation](#-citation)

</div>

</div>

<br/>

## ğŸ”¥ News

- **[02/09/2026]** ğŸš€ **Cross-Session Memory is Here â€” Outperforming Claude-Mem by 64%!** SimpleMem now supports **persistent memory across conversations**. On the LoCoMo benchmark, SimpleMem achieves a **64% performance boost** over Claude-Mem. Your agents can now recall context, decisions, and learnings from previous sessions automatically. [View Cross-Session Documentation â†’](cross/README.md)
- **[01/20/2026]** **SimpleMem is now available on PyPI!** ğŸ“¦ Install directly via `pip install simplemem`. [View Package Usage Guide â†’](docs/PACKAGE_USAGE.md)
- **[01/19/2026]** **Added Local Memory Storage for SimpleMem Skill!** ğŸ’¾ SimpleMem Skill now supports local memory storage within Claude Skills.
- **[01/18/2026]** **SimpleMem now supports Claude Skills!** ğŸš€ Use SimpleMem in claude.ai for long-term memory across conversations. Register at [mcp.simplemem.cloud](https://mcp.simplemem.cloud), configure your token, and import the skill!
- **[01/14/2026]** **SimpleMem MCP Server is now LIVE and Open Source!** ğŸ‰ Cloud-hosted memory service at [mcp.simplemem.cloud](https://mcp.simplemem.cloud). Integrates with LM Studio, Cherry Studio, Cursor, Claude Desktop via **Streamable HTTP** MCP protocol. [View MCP Documentation â†’](MCP/README.md)
- **[01/08/2026]** ğŸ”¥ Join our [Discord](https://discord.gg/KA2zC32M) and [WeChat Group](fig/wechat_logo3.JPG) to collaborate and exchange ideas!
- **[01/05/2026]** SimpleMem paper was released on [arXiv](https://arxiv.org/abs/2601.02553)!

---

## ğŸ“‘ Table of Contents

- [ğŸŒŸ Overview](#-overview)
- [ğŸ¯ Key Contributions](#-key-contributions)
- [ğŸš€ Performance Highlights](#-performance-highlights)
- [ğŸ“¦ Installation](#-installation)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ§  Cross-Session Memory](#-cross-session-memory)
- [ğŸ”Œ MCP Server](#-mcp-server)
- [ğŸ“Š Evaluation](#-evaluation)
- [ğŸ“ Citation](#-citation)
- [ğŸ“„ License](#-license)
- [ğŸ™ Acknowledgments](#-acknowledgments)

---

## ğŸŒŸ Overview

<div align="center">
<img src="fig/Fig_tradeoff.png" alt="Performance vs Efficiency Trade-off" width="900"/>

*SimpleMem achieves superior F1 score (43.24%) with minimal token cost (~550), occupying the ideal top-left position.*
</div>

**SimpleMem** is an efficient memory framework based on **semantic lossless compression** that addresses the fundamental challenge of **efficient long-term memory for LLM agents**. Unlike existing systems that either passively accumulate redundant context or rely on expensive iterative reasoning loops, SimpleMem maximizes **information density** and **token utilization** through a three-stage pipeline:

<table>
<tr>
<td width="33%" align="center">

### ğŸ” Stage 1
**Semantic Structured Compression**

Distills unstructured interactions into compact, multi-view indexed memory units

</td>
<td width="33%" align="center">

### ğŸ—‚ï¸ Stage 2
**Online Semantic Synthesis**

Intra-session process that instantly integrates related context into unified abstract representations to eliminate redundancy

</td>
<td width="33%" align="center">

### ğŸ¯ Stage 3
**Intent-Aware Retrieval Planning**

Infers search intent to dynamically determine retrieval scope and construct precise context efficiently

</td>
</tr>
</table>

<div align="center">
<img src="fig/Fig_framework.png" alt="SimpleMem Framework" width="900"/>

*The SimpleMem Architecture: (1) Semantic Structured Compression filters low-utility dialogue and converts informative windows into compact, context-independent memory units. (2) Online Semantic Synthesis consolidates related fragments during writing, maintaining a compact and coherent memory topology. (3) Intent-Aware Retrieval Planning infers search intent to adapt retrieval scope and query forms, enabling parallel multi-view retrieval and token-efficient context construction.*
</div>

---

### ğŸ† Performance Comparison

<div align="center">

**Speed Comparison Demo**

<video src="https://github.com/aiming-lab/SimpleMem/raw/main/fig/simplemem-new.mp4" controls width="900"></video>

*SimpleMem vs. Baseline: Real-time speed comparison demonstration*

</div>

<div align="center">

**LoCoMo-10 Benchmark Results (GPT-4.1-mini)**

| Model | â±ï¸ Construction Time | ğŸ” Retrieval Time | âš¡ Total Time | ğŸ¯ Average F1 |
|:------|:--------------------:|:-----------------:|:-------------:|:-------------:|
| A-Mem | 5140.5s | 796.7s | 5937.2s | 32.58% |
| LightMem | 97.8s | 577.1s | 675.9s | 24.63% |
| Mem0 | 1350.9s | 583.4s | 1934.3s | 34.20% |
| **SimpleMem** â­ | **92.6s** | **388.3s** | **480.9s** | **43.24%** |

</div>

> **ğŸ’¡ Key Advantages:**
> - ğŸ† **Highest F1 Score**: 43.24% (+26.4% vs. Mem0, +75.6% vs. LightMem)
> - âš¡ **Fastest Retrieval**: 388.3s (32.7% faster than LightMem, 51.3% faster than Mem0)
> - ğŸš€ **Fastest End-to-End**: 480.9s total processing time (12.5Ã— faster than A-Mem)

---

## ğŸ¯ Key Contributions

### 1ï¸âƒ£ Semantic Structured Compression

SimpleMem applies an **implicit semantic density gating** mechanism integrated into the LLM generation process to filter redundant interaction content. The system reformulates raw dialogue streams into **compact memory units** â€” self-contained facts with resolved coreferences and absolute timestamps. Each unit is indexed through three complementary representations for flexible retrieval:

<div align="center">

| ğŸ” Layer | ğŸ“Š Type | ğŸ¯ Purpose | ğŸ› ï¸ Implementation |
|---------|---------|------------|-------------------|
| **Semantic** | Dense | Conceptual similarity | Vector embeddings (1024-d) |
| **Lexical** | Sparse | Exact term matching | BM25-style keyword index |
| **Symbolic** | Metadata | Structured filtering | Timestamps, entities, persons |

</div>

**âœ¨ Example Transformation:**
```diff
- Input:  "He'll meet Bob tomorrow at 2pm"  [âŒ relative, ambiguous]
+ Output: "Alice will meet Bob at Starbucks on 2025-11-16T14:00:00"  [âœ… absolute, atomic]
```

---

### 2ï¸âƒ£ Online Semantic Synthesis

Unlike traditional systems that rely on asynchronous background maintenance, SimpleMem performs synthesis **on-the-fly during the write phase**. Related memory units are synthesized into higher-level abstract representations within the current session scope, allowing repetitive or structurally similar experiences to be **denoised and compressed immediately**.

**âœ¨ Example Synthesis:**
```diff
- Fragment 1: "User wants coffee"
- Fragment 2: "User prefers oat milk"
- Fragment 3: "User likes it hot"
+ Consolidated: "User prefers hot coffee with oat milk"
```

This proactive synthesis ensures the memory topology remains compact and free of redundant fragmentation.

---

### 3ï¸âƒ£ Intent-Aware Retrieval Planning

Instead of fixed-depth retrieval, SimpleMem leverages the reasoning capabilities of the LLM to generate a **comprehensive retrieval plan**. Given a query, the planning module infers **latent search intent** to dynamically determine retrieval scope and depth:

$$\{ q_{\text{sem}}, q_{\text{lex}}, q_{\text{sym}}, d \} \sim \mathcal{P}(q, H)$$

The system then executes **parallel multi-view retrieval** across semantic, lexical, and symbolic indexes, and merges results through ID-based deduplication:

<table>
<tr>
<td width="50%">

**ğŸ”¹ Simple Queries**
- Direct fact lookup via single memory unit
- Minimal retrieval depth
- Fast response time

</td>
<td width="50%">

**ğŸ”¸ Complex Queries**
- Aggregation across multiple events
- Expanded retrieval depth
- Comprehensive coverage

</td>
</tr>
</table>

**ğŸ“ˆ Result**: 43.24% F1 score with **30Ã— fewer tokens** than full-context methods.

---

## ğŸš€ Performance Highlights

### ğŸ“Š Benchmark Results (LoCoMo)

<details open>
<summary><b>ğŸ† Cross-Session Memory Comparison</b></summary>

| System | LoCoMo Score | vs SimpleMem |
|:-------|:------------:|:------------:|
| **SimpleMem** | **48** | â€” |
| Claude-Mem | 29.3 | **+64%** |

</details>

<details>
<summary><b>ğŸ”¬ High-Capability Models (GPT-4.1-mini)</b></summary>

| Task Type | SimpleMem F1 | Mem0 F1 | Improvement |
|:----------|:------------:|:-------:|:-----------:|
| **MultiHop** | 43.46% | 30.14% | **+43.8%** |
| **Temporal** | 58.62% | 48.91% | **+19.9%** |
| **SingleHop** | 51.12% | 41.3% | **+23.8%** |

</details>

<details>
<summary><b>âš™ï¸ Efficient Models (Qwen2.5-1.5B)</b></summary>

| Metric | SimpleMem | Mem0 | Notes |
|:-------|:---------:|:----:|:------|
| **Average F1** | 25.23% | 23.77% | Competitive with 99Ã— smaller model |

</details>

---

## ğŸ“¦ Installation

### ğŸ“ Notes for First-Time Users

- Ensure you are using **Python 3.10+ in your active environment**, not just installed globally.
- An OpenAI-compatible API key must be configured **before running any memory construction or retrieval**, otherwise initialization may fail.
- When using non-OpenAI providers (e.g., Qwen or Azure OpenAI), verify both the model name and `OPENAI_BASE_URL` in `config.py`.
- For large dialogue datasets, enabling parallel processing can significantly reduce memory construction time.

### ğŸ“‹ Requirements

- ğŸ Python 3.10+
- ğŸ“¦ [uv](https://docs.astral.sh/uv/) (recommended) or pip
- ğŸ”‘ OpenAI-compatible API (OpenAI, Qwen, Azure OpenAI, etc.)

### ğŸ› ï¸ Setup

```bash
# ğŸ“¥ Clone repository
git clone https://github.com/aiming-lab/SimpleMem.git
cd SimpleMem

# ğŸ“¦ Install dependencies (using uv â€” recommended)
uv sync

# Or with GPU support (PyTorch)
uv sync --extra gpu

# Or with evaluation metrics (for benchmarking)
uv sync --extra eval

# âš™ï¸ Configure API settings
cp config.py.example config.py
# Edit config.py with your API key and preferences
```

<details>
<summary><b>Alternative: pip install</b></summary>

```bash
pip install -r requirements.txt

# With GPU support
pip install -r requirements.txt -r requirements-gpu.txt
```

</details>

### âš™ï¸ Configuration Example

```python
# config.py
OPENAI_API_KEY = "your-api-key"
OPENAI_BASE_URL = None  # or custom endpoint for Qwen/Azure

LLM_MODEL = "gpt-4.1-mini"
EMBEDDING_MODEL = "Qwen/Qwen3-Embedding-0.6B"  # State-of-the-art retrieval
```

---

## âš¡ Quick Start

### ğŸ§  Understanding the Basic Workflow

At a high level, SimpleMem works as a long-term memory system for LLM-based agents. The workflow consists of three simple steps:

1. **Store information** â€“ Dialogues or facts are processed and converted into structured, atomic memories.
2. **Index memory** â€“ Stored memories are organized using semantic embeddings and structured metadata.
3. **Retrieve relevant memory** â€“ When a query is made, SimpleMem retrieves the most relevant stored information based on meaning rather than keywords.

This design allows LLM agents to maintain context, recall past information efficiently, and avoid repeatedly processing redundant history.

### ğŸ“ Basic Usage

```python
from main import SimpleMemSystem

# ğŸš€ Initialize system
system = SimpleMemSystem(clear_db=True)

# ğŸ’¬ Add dialogues (Stage 1: Semantic Structured Compression)
system.add_dialogue("Alice", "Bob, let's meet at Starbucks tomorrow at 2pm", "2025-11-15T14:30:00")
system.add_dialogue("Bob", "Sure, I'll bring the market analysis report", "2025-11-15T14:31:00")

# âœ… Finalize atomic encoding
system.finalize()

# ğŸ” Query with intent-aware retrieval (Stage 3: Intent-Aware Retrieval Planning)
answer = system.ask("When and where will Alice and Bob meet?")
print(answer)
# Output: "16 November 2025 at 2:00 PM at Starbucks"
```

---

### ğŸš„ Advanced: Parallel Processing

For large-scale dialogue processing, enable parallel mode:

```python
system = SimpleMemSystem(
    clear_db=True,
    enable_parallel_processing=True,  # âš¡ Parallel memory building
    max_parallel_workers=8,
    enable_parallel_retrieval=True,   # ğŸ” Parallel query execution
    max_retrieval_workers=4
)
```

> **ğŸ’¡ Pro Tip**: Parallel processing significantly reduces latency for batch operations!

---

## â“ Common Setup Issues & Troubleshooting

If you encounter issues while setting up or running SimpleMem for the first time, check the following common cases:

### 1ï¸âƒ£ API Key Not Detected
- Ensure your API key is correctly set in `config.py`
- For OpenAI-compatible providers (Qwen, Azure, etc.), verify that `OPENAI_BASE_URL` is configured correctly
- Restart your Python environment after updating the key

### 2ï¸âƒ£ Python Version Mismatch
- SimpleMem requires **Python 3.10+**
- Check your version using:
  ```bash
  python --version
  ```

---

## ğŸ§  Cross-Session Memory

**SimpleMem-Cross** extends SimpleMem with persistent cross-conversation memory capabilities. Agents can recall context, decisions, and observations from previous sessions â€” enabling continuity across conversations without manual context re-injection.

### Key Features

| Feature | Description |
|---------|-------------|
| **Session Lifecycle** | Full session management with start/record/stop/end lifecycle |
| **Automatic Context Injection** | Token-budgeted context from previous sessions injected at session start |
| **Event Collection** | Record messages, tool uses, file changes with automatic redaction |
| **Observation Extraction** | Heuristic extraction of decisions, discoveries, and learnings |
| **Provenance Tracking** | Every memory entry links back to source evidence |
| **Consolidation** | Decay, merge, and prune old memories to maintain quality |

### Quick Example

```python
from cross.orchestrator import create_orchestrator

async def main():
    orch = create_orchestrator(project="my-project")

    # Start session â€” previous context is injected automatically
    result = await orch.start_session(
        content_session_id="session-001",
        user_prompt="Continue building the REST API",
    )
    print(result["context"])  # Relevant context from previous sessions

    # Record events during the session
    await orch.record_message(result["memory_session_id"], "User asked about JWT")
    await orch.record_tool_use(
        result["memory_session_id"],
        tool_name="read_file",
        tool_input="auth/jwt.py",
        tool_output="class JWTHandler: ...",
    )

    # Finalize â€” extracts observations, generates summary, stores memories
    report = await orch.stop_session(result["memory_session_id"])
    print(f"Stored {report.entries_stored} memory entries")

    await orch.end_session(result["memory_session_id"])
    orch.close()
```

### Architecture

```
Agent Frameworks (Claude Code / Cursor / custom)
                    |
     +--------------+--------------+
     |                             |
Hook/Lifecycle Adapter      HTTP/MCP API (FastAPI)
     |                             |
     +--------------+--------------+
                    |
           CrossMemOrchestrator
                    |
  +-----------------+------------------+
  |                 |                  |
Session Manager  Context Injector  Consolidation
(SQLite)         (budgeted bundle) (decay/merge/prune)
  |                 |                  |
  +---------+-------+                  |
            |                          |
   Cross-Session Vector Store (LanceDB) <--+
```

### Module Reference

| Module | Description |
|--------|-------------|
| `cross/types.py` | Pydantic models, enums, records |
| `cross/storage_sqlite.py` | SQLite backend for sessions, events, observations |
| `cross/storage_lancedb.py` | LanceDB vector store with provenance |
| `cross/hooks.py` | Lifecycle hooks (SessionStart/ToolUse/End) |
| `cross/collectors.py` | Event collection with 3-tier redaction |
| `cross/session_manager.py` | Full session lifecycle orchestration |
| `cross/context_injector.py` | Token-budgeted context builder |
| `cross/orchestrator.py` | Top-level facade and factory |
| `cross/api_http.py` | FastAPI REST endpoints |
| `cross/api_mcp.py` | MCP tool definitions |
| `cross/consolidation.py` | Memory maintenance worker |

> ğŸ“– For detailed API documentation, see [Cross-Session README](cross/README.md)

---

## ğŸ”Œ MCP Server

SimpleMem is available as a **cloud-hosted memory service** via the Model Context Protocol (MCP), enabling seamless integration with AI assistants like Claude Desktop, Cursor, and other MCP-compatible clients.

**ğŸŒ Cloud Service**: [mcp.simplemem.cloud](https://mcp.simplemem.cloud)

### Key Features

| Feature | Description |
|---------|-------------|
| **Streamable HTTP** | MCP 2025-03-26 protocol with JSON-RPC 2.0 |
| **Multi-tenant Isolation** | Per-user data tables with token authentication |
| **Hybrid Retrieval** | Semantic search + keyword matching + metadata filtering |
| **Production Optimized** | Faster response times with OpenRouter integration |

### Quick Configuration

```json
{
  "mcpServers": {
    "simplemem": {
      "url": "https://mcp.simplemem.cloud/mcp",
      "headers": {
        "Authorization": "Bearer YOUR_TOKEN"
      }
    }
  }
}
```

> ğŸ“– For detailed setup instructions and self-hosting guide, see [MCP Documentation](MCP/README.md)

---

## ğŸ“Š Evaluation

### ğŸ§ª Run Benchmark Tests

```bash
# ğŸ¯ Full LoCoMo benchmark
python test_locomo10.py

# ğŸ“‰ Subset evaluation (5 samples)
python test_locomo10.py --num-samples 5

# ğŸ’¾ Custom output file
python test_locomo10.py --result-file my_results.json
```

---

### ğŸ”¬ Reproduce Paper Results

Use the exact configurations in `config.py`:
- **ğŸš€ High-capability**: GPT-4.1-mini, Qwen3-Plus
- **âš™ï¸ Efficient**: Qwen2.5-1.5B, Qwen2.5-3B
- **ğŸ” Embedding**: Qwen3-Embedding-0.6B (1024-d)

---

## ğŸ“ Citation

If you use SimpleMem in your research, please cite:

```bibtex
@article{simplemem2025,
  title={SimpleMem: Efficient Lifelong Memory for LLM Agents},
  author={Liu, Jiaqi and Su, Yaofeng and Xia, Peng and Zhou, Yiyang and Han, Siwei and  Zheng, Zeyu and Xie, Cihang and Ding, Mingyu and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2601.02553},
  year={2025},
  url={https://github.com/aiming-lab/SimpleMem}
}
```

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

We would like to thank the following projects and teams:

- ğŸ” **Embedding Model**: [Qwen3-Embedding](https://github.com/QwenLM/Qwen) - State-of-the-art retrieval performance
- ğŸ—„ï¸ **Vector Database**: [LanceDB](https://lancedb.com/) - High-performance columnar storage
- ğŸ“Š **Benchmark**: [LoCoMo](https://github.com/snap-research/locomo) - Long-context memory evaluation framework
